{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pickle\n",
    "import statistics\n",
    "import joblib\n",
    "import requests\n",
    "import uuid\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, learning_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from nltk import download, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, RSLPStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train = pd.read_excel('./1.data_raw/train.xlsx')\n",
    "airbnb_df_test = pd.read_excel('./1.data_raw/test.xlsx')\n",
    "airbnb_df_train_reviews = pd.read_excel('./1.data_raw/train_reviews.xlsx')\n",
    "airbnb_df_test_reviews = pd.read_excel('./1.data_raw/test_reviews.xlsx')\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "airbnb_df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train.loc[airbnb_df_train[\"host_about\"]==\":)\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GULIAS CODE HERE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Approach - TF - IDF with Multimodel for 3 main languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train_reviews[\"lang_comments\"] = airbnb_df_train_reviews[\"comments\"].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train_reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection for desc/host_about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train[\"lang_desc\"] = airbnb_df_train[\"description\"].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train[\"lang_host\"] = airbnb_df_train[\"host_about\"].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_df_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import detected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_detected = pd.read_csv(\"./2.data_detected/airbnb_df_train_detected.csv\", index_col=\"index\",).drop(\"Unnamed: 0\",axis=1)\n",
    "df_train_reviews_detected = pd.read_csv(\"./2.data_detected/airbnb_df_train__reviews_detected.csv\", index_col=\"index\").drop(\"Unnamed: 0\",axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count number of different Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_detected[\"lang_desc\"].value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_detected[\"lang_host\"].value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reviews_detected[\"lang_comments\"].value_counts()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df, df_review, language):\n",
    "\n",
    "    columns_to_drop = ['lang_desc', 'lang_host']\n",
    "    \n",
    "    df = df[(df['lang_desc'] == language) & (df['lang_host'] == language)]\n",
    "    df_review = df_review[df_review['lang_comments'] == language]\n",
    "    grouped_reviews = df_review.groupby('index')['comments'].apply(lambda x: ''.join(str(x))).reset_index()\n",
    "    merged_df = pd.merge(df, grouped_reviews, on='index', how='left')\n",
    "    merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "    merged_df = merged_df[[\"index\",\t\"description\", \"host_about\", \"comments\", \"unlisted\"]]\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join all English host_about/desc with English comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_english = create_df(df_train_detected, df_train_reviews_detected, \"en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_eng(row, tokenize, stop, lemmatize, stemmertize):\n",
    "    updates = []\n",
    "    \n",
    "    for j in tqdm(row):\n",
    "        \n",
    "        text = j\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        #REMOVE NUMERICAL DATA and PUNCTUATION\n",
    "        text = re.sub('<[^>]+>', ' ', text)\n",
    "        text = re.sub(\"[^a-zA-Z]\",\" \", text )\n",
    "\n",
    "        \n",
    "\n",
    "        if tokenize:\n",
    "            tokens = word_tokenize(text)\n",
    "            text = \" \".join(tokens)\n",
    "        \n",
    "        #REMOVE STOPWORDS\n",
    "        if stop:\n",
    "            stop_eng = set(stopwords.words('english'))\n",
    "            text = \" \".join([word for word in text.split() if word not in stop_eng])\n",
    "        \n",
    "        #Lemmatize\n",
    "        if lemmatize:\n",
    "            lemma_eng = WordNetLemmatizer()\n",
    "            text = \" \".join(lemma_eng.lemmatize(word) for word in text.split())\n",
    "            \n",
    "        \n",
    "        #Stemming\n",
    "        if stemmertize:\n",
    "            stemmer_eng = SnowballStemmer('english')\n",
    "            text = \" \".join(stemmer_eng.stem(word) for word in text.split())\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('wordnet')\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "columns_to_apply = ['description', 'host_about', 'comments']\n",
    "merged_df_english[columns_to_apply] = merged_df_english[columns_to_apply].astype(str).apply(lambda row: preprocessing_eng(row=row,\n",
    "                                                                                                                        tokenize=True,\n",
    "                                                                                                                        stop=True,\n",
    "                                                                                                                        lemmatize = True, \n",
    "                                                                                                                        stemmertize = False\n",
    "                                                                                                                        )\n",
    "                                                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_english.to_csv(\"./3.data_train/merged_df_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_english = pd.read_csv(\"./3.data_train/merged_df_english.csv\").drop([\"Unnamed: 0\", \"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_english"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join all French host_about/desc with French comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_french = create_df(df_train_detected, df_train_reviews_detected, \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_french"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fr(row, tokenize, stop, lemmatize, stemmertize):\n",
    "    updates = []\n",
    "    \n",
    "    for j in tqdm(row):\n",
    "        \n",
    "        text = j\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        #REMOVE NUMERICAL DATA and PUNCTUATION\n",
    "        text = re.sub(\"[^a-zA-Z]\",\" \", text )\n",
    "        text = re.sub(\"br\", \"\", text)\n",
    "\n",
    "        if tokenize:\n",
    "            tokens = word_tokenize(text, language=\"french\")\n",
    "            text = \" \".join(tokens)\n",
    " \n",
    "        #REMOVE STOPWORDS\n",
    "        if stop:\n",
    "            stop_fr = set(stopwords.words('french'))\n",
    "            text = \" \".join([word for word in text.split() if word not in stop_fr])\n",
    "        \n",
    "\n",
    "        #Lemmatize\n",
    "        if lemmatize:\n",
    "            lemma_fr = spacy.load(\"fr_core_news_md\")\n",
    "            doc = lemma_fr(text)\n",
    "            #for word in doc:\n",
    "            text = \" \".join(i.lemma_ for i in doc)\n",
    "        \n",
    "        #Stemming\n",
    "        if stemmertize:\n",
    "            stemmer_fr = SnowballStemmer('french')\n",
    "            stem_doc = stemmer_fr(text)\n",
    "            for word in stem_doc:\n",
    "                text = \" \".join(stemmer_fr.stem(word) for word in text.split())\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('wordnet')\n",
    "download('stopwords')\n",
    "columns_to_apply = ['description', 'host_about', 'comments']\n",
    "merged_df_french[columns_to_apply] = merged_df_french[columns_to_apply].astype(str).apply(lambda x: preprocessing_fr(row=x,\n",
    "                                                                                                                     tokenize=True,\n",
    "                                                                                                                     stop=True,\n",
    "                                                                                                                     lemmatize = True, \n",
    "                                                                                                                     stemmertize = False\n",
    "                                                                                                                    )\n",
    "                                                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_french.to_csv(\"./3.data_train/merged_df_french.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_french = pd.read_csv(\"./3.data_train/merged_df_french.csv\").drop([\"Unnamed: 0\", \"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_french"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## join all Portuguese host_about/desc with Portuguese comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pt = create_df(df_train_detected, df_train_reviews_detected, \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pt(row, tokenize, stop, lemmatize, stemmertize):\n",
    "    updates = []\n",
    "    \n",
    "    for j in tqdm(row):\n",
    "        \n",
    "        text = j\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        #REMOVE NUMERICAL DATA and PUNCTUATION\n",
    "        text = re.sub(\"[^a-zA-Z]\",\" \", text )\n",
    "        text = re.sub(\"br\", \"\", text)\n",
    "\n",
    "        if tokenize:\n",
    "            tokens = word_tokenize(text, language=\"portuguese\")\n",
    "            text = \" \".join(tokens)\n",
    "            \n",
    "        #REMOVE STOPWORDS\n",
    "        if stop:\n",
    "            stop_pt = set(stopwords.words('portuguese'))\n",
    "            text = \" \".join([word for word in text.split() if word not in stop_pt])\n",
    "            \n",
    "        #Lemmatize\n",
    "        if lemmatize:\n",
    "            lemma_pt = spacy.load(\"pt_core_news_sm\")\n",
    "            doc = lemma_pt(text)\n",
    "            text = \" \".join(i.lemma_ for i in doc)\n",
    "        \n",
    "        #Stemming\n",
    "        if stemmertize:\n",
    "            stemmer_pt = RSLPStemmer()\n",
    "            stem_doc = stemmer_pt(text)\n",
    "            for word in stem_doc:\n",
    "                text = \" \".join(stemmer_pt.stem(word) for word in text.split())\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('wordnet')\n",
    "download('stopwords')\n",
    "columns_to_apply = ['description', 'host_about', 'comments']\n",
    "merged_df_pt[columns_to_apply] = merged_df_pt[columns_to_apply].astype(str).apply(lambda row: preprocessing_pt(row=row,\n",
    "                                                                                                             tokenize=True,\n",
    "                                                                                                             stop=True,\n",
    "                                                                                                             lemmatize = True, \n",
    "                                                                                                             stemmertize = False\n",
    "                                                                                                            )\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pt.to_csv(\"./3.data_train/merged_df_pt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pt = pd.read_csv(\"./3.data_train/merged_df_pt.csv\").drop([\"Unnamed: 0\",\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_pt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF, Train and Evaluation of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(df):\n",
    "    # Concatenate text columns into a single column\n",
    "    df['Concatenated_Text'] = df['description'] + ' ' + df['host_about'] + ' ' + df['comments']\n",
    "\n",
    "    #separate features and taget\n",
    "    X = df['Concatenated_Text'].astype(str)\n",
    "    y = df['unlisted'].astype(int)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(input_dim, output_dim, embedding_dim, hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim + 1, embedding_dim, input_length=input_dim, mask_zero=True))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(hidden_units, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(input_dim, output_dim, embedding_dim, hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim + 1, embedding_dim, input_length=input_dim, mask_zero=True))\n",
    "    model.add(LSTM(hidden_units))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fit_evaluate(X,y,estimator):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    precision_avg = []\n",
    "    recall_avg = []\n",
    "    f1_avg = []\n",
    "    accuracy_avg = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, x_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "\n",
    "        \n",
    "        x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "        x_val_tfidf = tfidf.transform(x_val)\n",
    "\n",
    "        estimator.fit(x_train_tfidf,y_train)\n",
    "        \n",
    "        y_pred = estimator.predict(x_val_tfidf)\n",
    "\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "\n",
    "        precision_avg.append(precision)\n",
    "        recall_avg.append(recall)\n",
    "        f1_avg.append(f1)\n",
    "        accuracy_avg.append(accuracy)\n",
    "    \n",
    "    precision_avg = statistics.mean(precision_avg)\n",
    "    recall_avg = statistics.mean(recall_avg)\n",
    "    f1_avg = statistics.mean(f1_avg)\n",
    "    accuracy_avg = statistics.mean(accuracy_avg)\n",
    "\n",
    "    classes = np.unique(np.concatenate((y_val, y_pred)))\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(np.arange(len(classes)) + 0.5, classes)\n",
    "    plt.yticks(np.arange(len(classes)) + 0.5, classes)\n",
    "    plt.show()\n",
    "\n",
    "    return precision_avg, recall_avg, f1_avg, accuracy_avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eng, y_eng = create_target(merged_df_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_eng = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(comment) for comment in X_eng)\n",
    "padded_comments = pad_sequences(X_eng, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_eng = create_mlp(input_dim=max_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "mlp_eng.summary()\n",
    "mlp_eng.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_eng = create_lstm(input_dim=max_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "lstm_eng.summary()\n",
    "lstm_eng.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_eng, y_eng, rf_classifier_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_eng, y_eng, mlp_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_eng, y_eng, lstm_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_eng = TfidfVectorizer()\n",
    "x_train_tfidf_eng = tfidf_eng.fit_transform(X_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_eng.fit(x_train_tfidf_eng, y_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_eng.fit(x_train_tfidf_eng, y_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_eng.fit(x_train_tfidf_eng, y_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./5.Vectorizer/tfidf_vectorizer_eng.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_eng, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joblib.dump(rf_classifier_eng, \"./4.models/rf_classifier_eng.joblib\")\n",
    "joblib.dump(mlp_eng, \"./4.models/mlp_eng.joblib\")\n",
    "joblib.dump(lstm_eng, \"./4.models/lstm_eng.joblib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fr, y_fr = create_target(merged_df_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_fr = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(comment) for comment in X_fr)\n",
    "padded_comments = pad_sequences(X_fr, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_fr = create_mlp(input_dim=max_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "mlp_fr.summary()\n",
    "mlp_fr.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_fr = create_lstm(input_dim=max_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "lstm_fr.summary()\n",
    "lstm_fr.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_fr, y_fr, rf_classifier_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_fr, y_fr, mlp_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_fr, y_fr, lstm_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_fr = TfidfVectorizer()\n",
    "x_train_tfidf_fr = tfidf_fr.fit_transform(X_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_fr.fit(x_train_tfidf_fr, y_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_fr.fit(x_train_tfidf_fr, y_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_fr.fit(x_train_tfidf_fr, y_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./5.Vectorizer/tfidf_vectorizer_fr.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_fr, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_classifier_fr, \"./4.models/rf_classifier_fr.joblib\")\n",
    "joblib.dump(mlp_fr, \"./4.models/mlp_fr.joblib\")\n",
    "joblib.dump(lstm_fr, \"./4.models/lstm_fr.joblib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt, y_pt = create_target(merged_df_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_pt = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(comment) for comment in X_pt)\n",
    "padded_comments = pad_sequences(X_pt, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pt = create_mlp(input_dim=max_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "mlp_pt.summary()\n",
    "mlp_pt.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pt = create_lstm(input_dim=max_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "lstm_pt.summary()\n",
    "lstm_pt.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_pt, y_pt, rf_classifier_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_pt, y_pt, mlp_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_fit_evaluate(X_pt, y_pt, lstm_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pt = TfidfVectorizer()\n",
    "x_train_tfidf_pt = tfidf_pt.fit_transform(X_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_pt.fit(x_train_tfidf_pt, y_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pt.fit(x_train_tfidf_pt, y_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pt.fit(x_train_tfidf_pt, y_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./5.Vectorizer/tfidf_vectorizer_pt.pkl', 'wb') as file:\n",
    "    pickle.dump(tfidf_pt, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_classifier_pt, \"./4.models/rf_classifier_pt.joblib\")\n",
    "joblib.dump(mlp_pt, \"./4.models/mlp_pt.joblib\")\n",
    "joblib.dump(lstm_pt, \"./4.models/lstm_pt.joblib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_eng(text):\n",
    "    key = \"4aacdafc18474eb0accce6d24349ac62\"\n",
    "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "\n",
    "    location = \"westeurope\"\n",
    "\n",
    "    path = '/translate'\n",
    "    constructed_url = endpoint + path\n",
    "\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'to': 'en'\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "\n",
    "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "\n",
    "    return response[0][\"translations\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_row(row):\n",
    "    description = row[1]\n",
    "    host_about = row[2]\n",
    "    comments = row[3]\n",
    "\n",
    "    # Concatenate text columns into a single column\n",
    "    concatenated_text = str(description) + ' ' + str(host_about) + ' ' + str(comments)\n",
    "\n",
    "    return concatenated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_df(df, df_review):\n",
    "    grouped_reviews = df_review.groupby('index')['comments'].apply(lambda x: ''.join(str(x))).reset_index()\n",
    "    merged_df = pd.merge(df, grouped_reviews, on='index', how='left')\n",
    "    merged_df = merged_df[[\"index\",\t\"description\", \"host_about\", \"comments\"]]\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_df = merge_test_df(airbnb_df_test, airbnb_df_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(row):\n",
    "    columns_to_apply = ['description', 'host_about', 'comments']\n",
    "    lang_desc = detect_language(str(row[1]))\n",
    "    lang_host = detect_language(str(row[2]))\n",
    "    lang_review = detect_language(str(row[3]))\n",
    "\n",
    "    if (lang_desc == \"en\") & (lang_host == \"en\") & (lang_review == \"en\"):\n",
    "        row = preprocessing_eng(row=row,\n",
    "                                tokenize=True,\n",
    "                                stop=True,\n",
    "                                lemmatize = True, \n",
    "                                stemmertize = False\n",
    "                                )\n",
    "\n",
    "        row = create_feature_row(row)\n",
    "\n",
    "        with open('./5.Vectorizer/tfidf_vectorizer_eng.pkl', 'rb') as file:\n",
    "            tfidf_vectorizer_eng = pickle.load(file)\n",
    "\n",
    "        row_list = [row]\n",
    "        row = tfidf_vectorizer_eng.transform(row_list)\n",
    "\n",
    "        loaded_rf_eng = joblib.load(\"./4.models/rf_classifier_eng.joblib\")\n",
    "\n",
    "        return int(loaded_rf_eng.predict(row)[0])\n",
    "\n",
    "    elif (lang_desc == \"fr\") & (lang_host == \"fr\") & (lang_review == \"fr\"):\n",
    "        row = preprocessing_fr(row=row,\n",
    "                                tokenize=True,\n",
    "                                stop=True,\n",
    "                                lemmatize = True, \n",
    "                                stemmertize = False\n",
    "                                )\n",
    "                                \n",
    "        row = create_feature_row(row)\n",
    "\n",
    "        with open('./5.Vectorizer/tfidf_vectorizer_fr.pkl', 'rb') as file:\n",
    "            tfidf_vectorizer_fr = pickle.load(file)\n",
    "\n",
    "        row_list = [row]\n",
    "        row = tfidf_vectorizer_fr.transform(row_list)\n",
    "\n",
    "        loaded_rf_fr = joblib.load(\"./4.models/rf_classifier_fr.joblib\")\n",
    "\n",
    "        return int(loaded_rf_fr.predict(row)[0])\n",
    "\n",
    "    elif (lang_desc == \"pt\") & (lang_host == \"pt\") & (lang_review == \"pt\"):\n",
    "        row = preprocessing_pt(row=row,\n",
    "                                tokenize=True,\n",
    "                                stop=True,\n",
    "                                lemmatize = True, \n",
    "                                stemmertize = False\n",
    "                                )\n",
    "\n",
    "        row = create_feature_row(row)\n",
    "\n",
    "        with open('./5.Vectorizer/tfidf_vectorizer_pt.pkl', 'rb') as file:\n",
    "            tfidf_vectorizer_pt = pickle.load(file)\n",
    "\n",
    "        row_list = [row]\n",
    "        row = tfidf_vectorizer_pt.transform(row_list)\n",
    "\n",
    "        loaded_rf_pt = joblib.load(\"./4.models/rf_classifier_pt.joblib\")\n",
    "\n",
    "        return int(loaded_rf_pt.predict(row)[0])\n",
    "    else:\n",
    "        loaded_rf_eng = joblib.load(\"./4.models/rf_classifier_eng.joblib\")\n",
    "        row = create_feature_row(row)\n",
    "        row = translate_to_eng(row)\n",
    "        row = preprocessing_eng(row=row,\n",
    "                                tokenize=True,\n",
    "                                stop=True,\n",
    "                                lemmatize = True, \n",
    "                                stemmertize = False\n",
    "                                )\n",
    "                                \n",
    "        with open('./5.Vectorizer/tfidf_vectorizer_eng.pkl', 'rb') as file:\n",
    "            tfidf_vectorizer_eng = pickle.load(file)\n",
    "\n",
    "        row = tfidf_vectorizer_eng.transform(row)\n",
    "\n",
    "        return int(loaded_rf_eng.predict(row)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_df[\"prediction\"] = merged_test_df.apply(lambda row: prediction(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_df[\"prediction\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_df.to_csv(\"./6.Predictions/prediction_approach_1.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach 2.1 - Glove Embedding with subsample of data (reaching resource limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('<[^>]+>', ' ', text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous detected and stored Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reviews = pd.read_csv(\"/content/drive/MyDrive/Uni/Text Mining/2.data_detected/airbnb_df_train__reviews_detected.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/content/drive/MyDrive/Uni/Text Mining/2.data_detected/airbnb_df_train_detected.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two train Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_train,df_train_reviews, on=\"index\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df):\n",
    "    grouped_df = df.groupby('index').agg({\n",
    "        'description': 'first',\n",
    "        'host_about': 'first',\n",
    "        'comments': lambda x: ' '.join(x.astype(str).values),\n",
    "        'lang_comments': lambda x: ' '.join(x.astype(str).values),\n",
    "        \"unlisted\" : \"first\"\n",
    "    }).reset_index()\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Preprocessing of the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = prepare_df(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['description', 'host_about', 'comments']\n",
    "\n",
    "grouped_df[columns_to_clean] = grouped_df[columns_to_clean].applymap(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame\n",
    "df = grouped_df.sample(800, random_state=1)\n",
    "\n",
    "# Filter out rows with missing values in necessary columns\n",
    "df = df.dropna(subset=['description', 'host_about', 'comments'])\n",
    "\n",
    "# Extract the necessary columns\n",
    "text_data = df['description'] + ' ' + df['host_about'] + ' ' + df['comments']\n",
    "labels = df['unlisted']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "text_train, text_test, labels_train, labels_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text and convert it into sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = max([len(sentence.split()) for sentence in text_train])\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(text_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to have the same length\n",
    "padded_train = pad_sequences(sequences_train, maxlen=max_sequence_length, padding='post')\n",
    "padded_test = pad_sequences(sequences_test, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe word embeddings\n",
    "embeddings_index = {}\n",
    "with open('/content/drive/MyDrive/glove.6B/glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train = np.array([embedding_matrix[word] for word in padded_train])\n",
    "embedded_test = np.array([embedding_matrix[word] for word in padded_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_train_flat = embedded_train.reshape(embedded_train.shape[0], -1)\n",
    "embedded_test_flat = embedded_test.reshape(embedded_test.shape[0], -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fit_evaluate(classifier, X_train, y_train, X_val, y_val):\n",
    "    # Train the Random Forest classifier\n",
    "    classifier = RandomForestClassifier(random_state=42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = classifier.predict(X_val)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_val, predictions)\n",
    "    precision = precision_score(y_val, predictions)\n",
    "    recall = recall_score(y_val, predictions)\n",
    "    f1 = f1_score(y_val, predictions)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_2_1 = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_2_1_pred = custom_fit_evaluate(rf_classifier_2_1, embedded_train_flat, labels_train, embedded_test_flat, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2_1 = create_mlp(input_dim=max_sequence_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "mlp_2_1.summary()\n",
    "mlp_2_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2_1_pred = custom_fit_evaluate(rf_classifier_2_1, embedded_train_flat, labels_train, embedded_test_flat, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2_1 = create_lstm(input_dim=max_sequence_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "lstm_2_1.summary()\n",
    "lstm_2_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2_1_pred = custom_fit_evaluate(rf_classifier_2_1, embedded_train_flat, labels_train, embedded_test_flat, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, predictions):\n",
    "    classes = np.unique(np.concatenate((labels, predictions)))\n",
    "\n",
    "    # Create a confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False)\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xticks(np.arange(len(classes)) + 0.5, classes)\n",
    "    plt.yticks(np.arange(len(classes)) + 0.5, classes)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels_test, rf_classifier_2_1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels_test, mlp_2_1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels_test, lstm_2_1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embedded_train_flat\n",
    "del embedded_test_flat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Dataframe and preparing them for the Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"/content/drive/MyDrive/Uni/Text Mining/test.xlsx\")\n",
    "df_test_reviews = pd.read_excel(\"/content/drive/MyDrive/Uni/Text Mining/test_reviews.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.merge(df_test,df_test_reviews, on=\"index\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_df = test_df.groupby('index').agg({\n",
    "        'description': 'first',\n",
    "        'host_about': 'first',\n",
    "        'comments': lambda x: ' '.join(x.astype(str).values),\n",
    "    }).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(classifier, row):\n",
    "    # Clean the text in the necessary columns\n",
    "    cleaned_text = row[[\"description\", \"host_about\", \"comments\"]].apply(clean_text)\n",
    "\n",
    "    # Combine the necessary columns into a single text\n",
    "    text_data = cleaned_text['description'] + ' ' + cleaned_text['host_about'] + ' ' + cleaned_text['comments']\n",
    "\n",
    "    # Tokenize and convert the text data into sequences\n",
    "    sequences = tokenizer.texts_to_sequences([text_data])\n",
    "\n",
    "    # Pad the sequences to have the same length as the training data\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Create word embedding feature vectors\n",
    "    embedded_data = np.array([embedding_matrix[word] for word in padded_sequences])\n",
    "    embedded_data_flat = embedded_data.reshape(embedded_data.shape[0], -1)\n",
    "\n",
    "    # Predict on the data\n",
    "    prediction = classifier.predict(embedded_data_flat)\n",
    "\n",
    "    return prediction[0]  # Assuming you want to return a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predictions'] = test_df.apply(lambda row: predict_row(rf_classifier_2_1, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"./6.Predictions/prediction_approach_2_1.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Approach 2.2 - Glove Embedding batch wise to avoid resource limitations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous detected and stored Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_reviews = pd.read_csv(\"/content/drive/MyDrive/Uni/Text Mining/2.data_detected/airbnb_df_train__reviews_detected.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/content/drive/MyDrive/Uni/Text Mining/2.data_detected/airbnb_df_train_detected.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_train,df_train_reviews, on=\"index\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = prepare_df(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['description', 'host_about', 'comments']\n",
    "\n",
    "grouped_df[columns_to_clean] = grouped_df[columns_to_clean].applymap(clean_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame\n",
    "df = grouped_df\n",
    "\n",
    "# Filter out rows with missing values in necessary columns\n",
    "df = df.dropna(subset=['description', 'host_about', 'comments'])\n",
    "\n",
    "# Extract the necessary columns\n",
    "text_data = df['description'] + ' ' + df['host_about'] + ' ' + df['comments']\n",
    "labels = df['unlisted']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "text_train, text_test, labels_train, labels_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text and convert it into sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = max([len(sentence.split()) for sentence in text_train])\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(text_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe word embeddings\n",
    "embeddings_index = {}\n",
    "with open('/content/drive/MyDrive/glove.6B/glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_2_2 = RandomForestClassifier(random_state=42, n_estimators=300, min_samples_split=32, min_samples_leaf=16, max_depth=4, warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2_2 = create_mlp(input_dim=max_sequence_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "mlp_2_2.summary()\n",
    "mlp_2_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2_2 = create_lstm(input_dim=max_sequence_length, output_dim=1, embedding_dim=50, hidden_units=64)\n",
    "lstm_2_2.summary()\n",
    "lstm_2_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_model(classifier, sequences_train, labels_train, batch_size):\n",
    "    # Get the total number of samples\n",
    "    total_samples = len(sequences_train)\n",
    "\n",
    "\n",
    "    # Iterate over the data in batches\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        # Get the current batch\n",
    "        batch_start = i\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch_sequences_train = sequences_train[batch_start:batch_end]\n",
    "\n",
    "        # Pad sequences to have the same length\n",
    "        padded_train = pad_sequences(batch_sequences_train, maxlen=max_sequence_length, padding='post')\n",
    "        # Create word embedding feature vectors\n",
    "        embedded_train = np.array([embedding_matrix[word] for word in padded_train])\n",
    "        embedded_train_flat = embedded_train.reshape(embedded_train.shape[0], -1)\n",
    "\n",
    "        classifier.fit(embedded_train_flat, labels_train[batch_start:batch_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(rf_classifier_2_2, sequences_train, labels_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(mlp_2_2, sequences_train, labels_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(lstm_2_2, sequences_train, labels_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, sequences_test, labels_test, batch_size):\n",
    "    total_samples = len(sequences_test)\n",
    "\n",
    "    # Initialize an empty array to store the predictions\n",
    "    predictions = np.array([])\n",
    "\n",
    "    # Iterate over the data in batches\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        # Get the current batch\n",
    "        batch_start = i\n",
    "        batch_end = min(i + batch_size, total_samples)\n",
    "        batch_sequences_test = sequences_test[batch_start:batch_end]\n",
    "        padded_test = pad_sequences(batch_sequences_test, maxlen=max_sequence_length, padding='post')\n",
    "        embedded_test = np.array([embedding_matrix[word] for word in padded_test])\n",
    "        embedded_test_flat = embedded_test.reshape(embedded_test.shape[0], -1)\n",
    "        batch_predictions = classifier.predict(embedded_test_flat)\n",
    "\n",
    "        predictions = np.concatenate((predictions, batch_predictions))\n",
    "\n",
    "    accuracy = accuracy_score(labels_test, predictions)\n",
    "    precision = precision_score(labels_test, predictions)\n",
    "    recall = recall_score(labels_test, predictions)\n",
    "    f1 = f1_score(labels_test, predictions)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier_2_2_pred = evaluate_model(rf_classifier_2_2, sequences_test, labels_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_2_2_pred = evaluate_model(mlp_2_2, sequences_test, labels_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_2_2_pred = evaluate_model(lstm_2_2, sequences_test, labels_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels_test, rf_classifier_2_2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels_test, mlp_2_2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels_test, lstm_2_2_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Dataframe and preparing for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"/content/drive/MyDrive/Uni/Text Mining/test.xlsx\")\n",
    "df_test_reviews = pd.read_excel(\"/content/drive/MyDrive/Uni/Text Mining/test_reviews.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.merge(df_test,df_test_reviews, on=\"index\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_df = test_df.groupby('index').agg({\n",
    "        'description': 'first',\n",
    "        'host_about': 'first',\n",
    "        'comments': lambda x: ' '.join(x.astype(str).values),\n",
    "    }).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_row(classifier, row):\n",
    "    # Clean the text in the necessary columns\n",
    "    cleaned_text = row[[\"description\", \"host_about\", \"comments\"]].apply(clean_text)\n",
    "\n",
    "    # Combine the necessary columns into a single text\n",
    "    text_data = cleaned_text['description'] + ' ' + cleaned_text['host_about'] + ' ' + cleaned_text['comments']\n",
    "\n",
    "    # Tokenize and convert the text data into sequences\n",
    "    sequences = tokenizer.texts_to_sequences([text_data])\n",
    "\n",
    "    # Pad the sequences to have the same length as the training data\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    # Create word embedding feature vectors\n",
    "    embedded_data = np.array([embedding_matrix[word] for word in padded_sequences])\n",
    "    embedded_data_flat = embedded_data.reshape(embedded_data.shape[0], -1)\n",
    "\n",
    "    # Predict on the data\n",
    "    prediction = classifier.predict(embedded_data_flat)\n",
    "\n",
    "    return prediction[0]  # Assuming you want to return a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predictions'] = test_df.apply(lambda row: predict_row(rf_classifier_2_2, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['predictions'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Approach - Transformer based Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project-aNjsu91Q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
